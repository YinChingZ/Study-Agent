"""
StudyAgent LLM å·¥å‚æ¨¡å—

æ ¹æ®é…ç½®ä¿¡æ¯åˆ›å»ºä¸åŒæä¾›å•†çš„ LLM å®ä¾‹ã€‚
"""

import logging
import os

from browser_use.llm import ChatOpenAI, ChatAnthropic
from browser_use.llm.google.chat import ChatGoogle
from browser_use.llm.base import BaseChatModel

from study_agent.config import LLMConfig, AppConfig

logger = logging.getLogger("study_agent")


def _create_openai_llm(config: LLMConfig) -> ChatOpenAI:
    """åˆ›å»º OpenAI LLM å®ä¾‹ã€‚

    å½“ç¯å¢ƒå˜é‡ OPENAI_NO_STRUCTURED_OUTPUT=true æ—¶ï¼Œç¦ç”¨ json_schema ç»“æ„åŒ–è¾“å‡ºï¼Œ
    æ”¹ä¸ºå°† schema æ³¨å…¥ç³»ç»Ÿæç¤ºè¯ã€‚é€‚ç”¨äºä¸æ”¯æŒ response_format: json_schema çš„ç¬¬ä¸‰æ–¹ APIã€‚
    """
    model = config.model or os.getenv("OPENAI_MODEL", "gpt-4o")
    base_url = config.base_url or os.getenv("OPENAI_BASE_URL", None)

    kwargs: dict = {"model": model}
    if base_url:
        kwargs["base_url"] = base_url
    if config.max_completion_tokens is not None:
        kwargs["max_completion_tokens"] = config.max_completion_tokens

    # å…¼å®¹ä¸æ”¯æŒ json_schema ç»“æ„åŒ–è¾“å‡ºçš„ç¬¬ä¸‰æ–¹ API
    no_structured = os.getenv("OPENAI_NO_STRUCTURED_OUTPUT", "false").lower() in (
        "true", "1", "yes",
    )
    if no_structured:
        kwargs["dont_force_structured_output"] = True
        kwargs["add_schema_to_system_prompt"] = True
        logger.info("âš™ï¸ å·²ç¦ç”¨ json_schema ç»“æ„åŒ–è¾“å‡ºï¼Œæ”¹ä¸º schema-in-prompt æ¨¡å¼")

    return ChatOpenAI(**kwargs)


def _create_anthropic_llm(config: LLMConfig) -> ChatAnthropic:
    """åˆ›å»º Anthropic LLM å®ä¾‹ã€‚"""
    model = config.model or os.getenv("ANTHROPIC_MODEL", "claude-sonnet-4-20250514")
    return ChatAnthropic(model=model)


def _create_google_llm(config: LLMConfig) -> ChatGoogle:
    """åˆ›å»º Google LLM å®ä¾‹ã€‚"""
    model = config.model or os.getenv("GOOGLE_MODEL", "gemini-2.0-flash")
    return ChatGoogle(model=model)


_FACTORY_MAP = {
    "openai": _create_openai_llm,
    "anthropic": _create_anthropic_llm,
    "google": _create_google_llm,
}


def create_llm(config: LLMConfig) -> BaseChatModel:
    """æ ¹æ® LLMConfig åˆ›å»ºå¯¹åº”æä¾›å•†çš„ LLM å®ä¾‹ã€‚"""
    factory = _FACTORY_MAP.get(config.provider)
    if factory is None:
        raise ValueError(f"ä¸æ”¯æŒçš„ Provider: {config.provider}")
    return factory(config)


def create_llm_pair(app_config: AppConfig) -> tuple[BaseChatModel, BaseChatModel]:
    """åˆ›å»º Browser Agent LLM å’Œ Solver LLMï¼Œå¹¶æ‰“å°é…ç½®ä¿¡æ¯ã€‚"""
    bc = app_config.browser_llm
    sc = app_config.solver_llm

    print(f'ğŸ¤– Browser Agent: {bc.provider.upper()} (Model: {bc.model or "Default"})')
    if bc.base_url:
        print(f"   API Base: {bc.base_url}")

    print(f'ğŸ§  Solver Agent: {sc.provider.upper()} (Model: {sc.model or "Default"})')
    if sc.base_url:
        print(f"   API Base: {sc.base_url}")

    browser_llm = create_llm(bc)
    solver_llm = create_llm(sc)
    return browser_llm, solver_llm
